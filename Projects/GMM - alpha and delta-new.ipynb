{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following is code to estimate capital depreciaion and adjustment costs in the 2 country, 2 sector model in EKNR (2016)\n",
    "import os\n",
    "os.getcwd()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "k_raw = pd.read_excel(\"Data/BEA_tab_11_k_stock.xls\")\n",
    "k_raw.head()\n",
    "k_raw = k_raw.drop([0,1,2,3])\n",
    "k_raw.head()\n",
    "k_raw.columns = k_raw.loc[4,:]\n",
    "k_raw = k_raw.drop([4, 5])\n",
    "\n",
    "#keep only fixed assets and consumer durable goods, fixed assets, private, government, and consumer durbales\n",
    "#index: 6, 7, 8, 14, 20\n",
    "list_to_keep = [6, 7, 8, 14, 20]\n",
    "k_raw = k_raw.loc[list_to_keep]\n",
    "# k_raw = k_raw.drop(['Line'], axis=1)\n",
    "k_raw =k_raw.reset_index()\n",
    "k_raw = k_raw.drop(['index'], axis=1)\n",
    "k_raw.rename(columns={np.nan:'type'}, inplace=True)\n",
    "\n",
    "#reformat and create changes, which is (t+1)/(t)\n",
    "k_raw = k_raw.T\n",
    "k_raw.columns = k_raw.loc['type',:]\n",
    "k_raw = k_raw.drop(['Line', 'type'], axis = 0)\n",
    "k_data_hat =k_raw.div(k_raw.shift(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "inv_raw = pd.read_excel(\"Data/BEA_tab_15_inv.xls\")\n",
    "inv_raw = inv_raw.drop([0,1,2,3])\n",
    "inv_raw.columns = inv_raw.loc[4,:]\n",
    "inv_raw = inv_raw.drop([4, 5])\n",
    "\n",
    "\n",
    "inv_raw = inv_raw.loc[list_to_keep]\n",
    "inv_raw =inv_raw.reset_index()\n",
    "inv_raw = inv_raw.drop(['index'], axis=1)\n",
    "inv_raw.rename(columns={np.nan:'type'}, inplace=True)\n",
    "\n",
    "#reformat and create pct changes\n",
    "inv_raw = inv_raw.T\n",
    "inv_raw.columns = inv_raw.loc['type',:]\n",
    "inv_raw = inv_raw.drop(['Line', 'type'], axis = 0)\n",
    "inv_data_hat =inv_raw.div(inv_raw.shift(1))\n",
    "inv_data_hat.columns = ['Fixed_assets_and_cons_durables', 'Fixed_assets', 'Private', 'Government', 'Consumer_durables']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#US GDP data in dollars\n",
    "#note that i have predicted values for durables and services directly in xls file for 2000 and 2001\n",
    "#I did this for cells c:10 - d:13\n",
    "GDP_raw = pd.read_excel(\"Data/BEA_tab_116_GDP_USD.xls\")\n",
    "GDP_raw = GDP_raw.drop([0,1,2,3])\n",
    "GDP_raw.columns = GDP_raw.loc[4,:]\n",
    "GDP_raw = GDP_raw.drop([4, 5])\n",
    "GDP_raw\n",
    "list_to_keep = [6, 9, 11, 12, 27]\n",
    "GDP_raw = GDP_raw.loc[list_to_keep]\n",
    "\n",
    "GDP_raw =GDP_raw.reset_index()\n",
    "GDP_raw = GDP_raw.drop(['index'], axis=1)\n",
    "GDP_raw.rename(columns={np.nan:'type'}, inplace=True)\n",
    "\n",
    "GDP_raw = GDP_raw.T\n",
    "GDP_raw.columns = GDP_raw.loc['type',:]\n",
    "GDP_raw = GDP_raw.drop(['Line', 'type'], axis = 0)\n",
    "GDP_data_hat =GDP_raw.div(GDP_raw.shift(1))\n",
    "\n",
    "GDP_data_hat.head()\n",
    "GDP_data_hat.columns = [\"GDP\", \"Durables\", \"Services\", \"Gross_private_domestic_investment\", \"Govt_cons_exp_and_gross_inv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load investment efficiency in changes\n",
    "chi_data_hat = pd.read_excel(\"Data/inv_eff_EKNR.xlsx\")\n",
    "chi_data_hat_old = chi_data_hat\n",
    "\n",
    "#load trade cost shocks\n",
    "d_data_hat = pd.read_excel(\"Data/d_EKNR.xlsx\")\n",
    "\n",
    "A_data_hat = pd.read_excel(\"Data/productivity_EKNR.xlsx\")\n",
    "A_data_hat = A_data_hat.drop([0])\n",
    "A_data_hat.columns = ['Year', 'country_1', 'country_2']\n",
    "#read in pi_ijnt\n",
    "pi_ijt_obs = pd.read_csv(\"Data/pi_ijt.csv\")\n",
    "pi_ijt_obs = pi_ijt_obs.drop(['Unnamed: 0'], axis=1)\n",
    "# k_raw = k_raw.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex both\n",
    "# new_index = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "#             '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "k_data_hat = k_data_hat.reset_index()\n",
    "GDP_data_hat = GDP_data_hat.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_data_hat.columns = ['Years', 'Fixed_assets_and_cons_durables',\n",
    "                         'Fixed_assets',                        'Private',\n",
    "                           'Government',              'Consumer_durables']\n",
    "GDP_data_hat.columns = ['Years', 'GDP',\n",
    "                                'Durables',\n",
    "                                'Services',\n",
    "       'Gross_private_domestic_investment',\n",
    "             'Govt_cons_exp_and_gross_inv']\n",
    "k_data_hat_old = k_data_hat #for use in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide what you want to use\n",
    "#using total GDP for Y in calculating factors, and consumer durables only for K\n",
    "# using consumer durables only for inv. spending\n",
    "#use only durable GDP for Y when simulating data for beta l and betak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###OPTION 1: construct betaL, betaK myself\n",
    "#do this for annual data\n",
    "#make the y, l, m, k data here (all for durables only)\n",
    "#run in stata and find beta_L_orig, beta_K_orig\n",
    "#repeat below\n",
    "\n",
    "##Log inputs (quantity index)\n",
    "int_inputs_raw = pd.read_excel(\"Data/BEA_quantity_int_inp_durables.xls\")\n",
    "list_to_keep = [4,5]\n",
    "int_inputs_raw = int_inputs_raw.loc[list_to_keep,:]\n",
    "int_inputs_raw.columns = int_inputs_raw.loc[4, :]\n",
    "int_inputs_raw = int_inputs_raw.drop([4])\n",
    "int_inputs_raw = int_inputs_raw.drop(['Line', np.nan], axis=1)\n",
    "int_inputs_raw = int_inputs_raw.T\n",
    "int_inputs_raw['Years'] = int_inputs_raw.index\n",
    "int_inputs_raw.columns = ['m', 'Years']\n",
    "int_inputs_raw = int_inputs_raw.reset_index()\n",
    "int_inputs_raw = int_inputs_raw.drop([4], axis = 1)\n",
    "int_inputs_log =np.log(int_inputs_raw['m'])\n",
    "int_inputs_log = pd.DataFrame(int_inputs_log)\n",
    "int_inputs_log\n",
    "int_inputs_log['Years']= int_inputs_raw.Years\n",
    "int_inputs_log.columns = ['m_log', 'Years']\n",
    "\n",
    "# ask jonas about units here! does it matter?\n",
    "##log Durables - GDP (billions)\n",
    "log_y_durables_data = pd.Series(np.log(GDP_raw.iloc[:,1].values.astype(float)))\n",
    "\n",
    "##log labor (labor is in 1000s)\n",
    "labor_raw = pd.read_excel(\"Data/FRED_emp.xls\")\n",
    "labor_log = pd.Series(np.log(labor_raw['Emp_thousands']))\n",
    "\n",
    "##log K (billions)\n",
    "log_K_durables_data = pd.Series(np.log(k_raw.iloc[:,4].values.astype(float)))\n",
    "\n",
    "lists_for_prod = [pd.DataFrame(int_inputs_log['m_log']),\n",
    "                  pd.DataFrame(log_y_durables_data),\n",
    "                  pd.DataFrame(labor_log),\n",
    "                 pd.DataFrame(log_K_durables_data)]\n",
    "prod_df = pd.concat(lists_for_prod, axis=1)\n",
    "prod_df.columns = ['m_log', 'y_log', 'l_log', 'K_log']\n",
    "prod_df.to_csv('Data/prod_data_US.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###OPTION 1A: construct betaL, betaK myself\n",
    "#do this for specific categories of consumer durables\n",
    "#make the y, l, m, k data here (all for durables only)\n",
    "#run in stata and find beta_L_orig, beta_K_orig\n",
    "#repeat below\n",
    "int_inputs_detail_raw = pd.read_excel(\"Data/detail_prod_data/BEA_m_detail.xls\")\n",
    "list_to_keep = [4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.loc[list_to_keep,:]\n",
    "int_inputs_detail_raw.columns = int_inputs_detail_raw.loc[4, :]\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.drop([4])\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.drop(['Line'], axis=1)\n",
    "int_inputs_detail_raw.columns = ['category', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "             '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.reset_index()\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.drop(['index'], axis=1)\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.T\n",
    "int_inputs_detail_raw.columns = int_inputs_detail_raw.iloc[0,:]\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.drop(['category'])\n",
    "\n",
    "GDP_detail_raw = pd.read_excel(\"Data/detail_prod_data/BEA_GDP_detail.xls\")\n",
    "list_to_keep = [4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "GDP_detail_raw = GDP_detail_raw.loc[list_to_keep,:]\n",
    "GDP_detail_raw.columns = GDP_detail_raw.loc[4, :]\n",
    "GDP_detail_raw = GDP_detail_raw.drop([4])\n",
    "GDP_detail_raw = GDP_detail_raw.drop(['Line'], axis=1)\n",
    "GDP_detail_raw.columns = ['category', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "             '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "GDP_detail_raw = GDP_detail_raw.reset_index()\n",
    "GDP_detail_raw = GDP_detail_raw.drop(['index'], axis=1)\n",
    "GDP_detail_raw = GDP_detail_raw.T\n",
    "GDP_detail_raw.columns = GDP_detail_raw.iloc[0,:]\n",
    "GDP_detail_raw = GDP_detail_raw.drop(['category'])\n",
    "GDP_detail_raw.columns = int_inputs_detail_raw.columns\n",
    "\n",
    "Labor_detail_raw = pd.read_excel(\"Data/detail_prod_data/BEA_labor_detail.xls\")\n",
    "list_to_keep = [4,5,6,7,8,9,10,11,12,13,14,15,16, 17, 18]\n",
    "Labor_detail_raw = Labor_detail_raw.loc[list_to_keep,:]\n",
    "Labor_detail_raw.columns = Labor_detail_raw.loc[4, :]\n",
    "Labor_detail_raw = Labor_detail_raw.drop([4,5,6])\n",
    "Labor_detail_raw = Labor_detail_raw.drop(['Line'], axis=1)\n",
    "Labor_detail_raw.rename(columns={np.nan:'category'}, inplace=True)\n",
    "Labor_detail_raw = Labor_detail_raw.T\n",
    "Labor_detail_raw = Labor_detail_raw.reset_index()\n",
    "Labor_detail_raw.columns = Labor_detail_raw.iloc[0,:]\n",
    "Labor_detail_raw = Labor_detail_raw.drop([0])\n",
    "Labor_detail_raw.index = Labor_detail_raw.category\n",
    "Labor_detail_raw = Labor_detail_raw.drop(['category'], axis=1)\n",
    "Labor_detail_raw.columns = int_inputs_detail_raw.columns\n",
    "\n",
    "#note this raw file is in millions, whereas before it was in billions\n",
    "#had to windsorize this due to \"less treasury stock\" from FRED and unavailability of matching categories\n",
    "K_detail_raw = pd.read_excel(\"Data/detail_prod_data/FRED_k_Detail_windsorized_new.xlsx\")\n",
    "K_detail_raw.index = K_detail_raw.Years\n",
    "K_detail_raw = K_detail_raw.drop(['Years'], axis=1)\n",
    "K_detail_raw.columns = Labor_detail_raw.columns\n",
    "K_detail_raw = K_detail_raw/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year 2000 from labor, GDP, int_inputs because K is lacking this year\n",
    "Labor_detail_raw = Labor_detail_raw.drop([2000.0])\n",
    "int_inputs_detail_raw = int_inputs_detail_raw.drop(['2000'])\n",
    "GDP_detail_raw = GDP_detail_raw.drop(['2000'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now take logs and merge into a df\n",
    "GDP_detail_log = pd.DataFrame(np.log(GDP_detail_raw.values.astype(float)))\n",
    "GDP_detail_log.columns = Labor_detail_raw.columns\n",
    "\n",
    "int_inputs_detail_log = pd.DataFrame(np.log(int_inputs_detail_raw.values.astype(float)))\n",
    "int_inputs_detail_log.columns = Labor_detail_raw.columns\n",
    "\n",
    "K_detail_log =  pd.DataFrame(np.log(K_detail_raw.values.astype(float)))\n",
    "K_detail_log.columns = Labor_detail_raw.columns\n",
    "\n",
    "Labor_detail_log = pd.DataFrame(np.log(Labor_detail_raw.values.astype(float)))\n",
    "Labor_detail_log.columns = Labor_detail_raw.columns\n",
    "# np.log(GDP_raw.iloc[:,1].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append everything by column type\n",
    "list_columns = Labor_detail_raw.columns\n",
    "prod_df_ver_1a = []\n",
    "index = 0\n",
    "for i in list_columns:\n",
    "    index +=1\n",
    "    index_str = str(index)\n",
    "    lists_for_prod_ver_1a = [pd.DataFrame(int_inputs_detail_log[i]),\n",
    "                    pd.DataFrame(GDP_detail_log[i]),\n",
    "                        pd.DataFrame(Labor_detail_log[i]),\n",
    "                        pd.DataFrame(K_detail_log[i])]\n",
    "    df_to_add = pd.concat(lists_for_prod_ver_1a, axis=1)\n",
    "    df_to_add.columns = ['m_log', 'y_log', 'l_log', 'K_log']\n",
    "    prod_df_ver_1a.append(df_to_add)\n",
    "#     df_to_add.to_csv('Data/prod_data_ver_1a/type' + index_str+'.csv')\n",
    "    df_to_add.to_excel('Data/prod_data_ver_1a/type' + index_str+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now write everything as functions tht take beta as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_b_hat(beta_L_for_calc, beta_K_for_calc):\n",
    "    #now construct b_hat\n",
    "    ##For EKNR formulation, assume betal = 2/3, betak = 1/3,\n",
    "    ##For my formulation, betal =  .6670052732854, betak = .2572608033009\n",
    "    #b_hat = (Yhat/Lhat)^beta_L * (Yhat/Khat)^beta_K\n",
    "    L_hat = 1 #constant\n",
    "    b_hat = (GDP_data_hat.GDP/L_hat)**beta_L_for_calc * (GDP_data_hat.GDP/k_data_hat.Consumer_durables)**beta_K_for_calc\n",
    "    return (b_hat)\n",
    "    #notice b_hat is 2000, 2001, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now construct p_hat_D\n",
    "#choose 2 countries of interest\n",
    "# for now, US and China\n",
    "# these are indices \n",
    "country_full = [\n",
    "  \"Australia\",\n",
    "  \"Austria\",\n",
    "  # \"Belgium\",\n",
    "  # \"Bulgaria\",\n",
    "  \"Brazil\",\n",
    "  \"Canada\",\n",
    "  \"Switzerland\",\n",
    "  \"China\",\n",
    "  # \"Cyprus\",\n",
    "  # \"Czechia\",\n",
    "  \"Germany\",\n",
    "  # \"Denmark\",\n",
    "  \"Spain\",\n",
    "  # \"Estonia\",\n",
    "  \"Finland\",\n",
    "  \"France\",\n",
    "  \"United Kingdom\",\n",
    "  \"Greece\",\n",
    "  # \"Croatia\",\n",
    "  # \"Hungary\",\n",
    "  \"Indonesia\",\n",
    "  \"India\",\n",
    "  # \"Ireland\",\n",
    "  \"Italy\",\n",
    "  \"Japan\",\n",
    "  \"Korea, Republic of\",\n",
    "  # \"Lithuania\",\n",
    "  # \"Luxembourg\",\n",
    "  # \"Latvia\",\n",
    "  \"Mexico\",\n",
    "  # \"Malta\",\n",
    "  \"Netherlands\",\n",
    "  \"Norway\",\n",
    "  \"Poland\",\n",
    "  \"Portugal\",\n",
    "  \"Romania\",\n",
    "  \"Russian Federation\",\n",
    "  \"Slovakia\",\n",
    "  \"Slovenia\",\n",
    "  \"Sweden\",\n",
    "  \"Turkey\",\n",
    "  # \"Taiwan, Province of China\",\n",
    "  \"United States\",\n",
    "  \"ROW\"\n",
    "]\n",
    "c1_ind = country_full.index('United States') #28\n",
    "c2_ind = country_full.index('China') #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pi data\n",
    "pi_subset  = pi_ijt_obs[(pi_ijt_obs['Country1'] == c1_ind) | (pi_ijt_obs['Country2'] == c1_ind)]\n",
    "pi_subset = pi_subset[(pi_subset['Country1'] == c2_ind) | (pi_subset['Country2'] == c2_ind)]\n",
    "pi_ii_subset  = pi_ijt_obs[(pi_ijt_obs['Country1'] == c1_ind) & (pi_ijt_obs['Country2'] == c1_ind)]\n",
    "pi_jj_subset  = pi_ijt_obs[(pi_ijt_obs['Country1'] == c2_ind) & (pi_ijt_obs['Country2'] == c2_ind)]\n",
    "\n",
    "frames = [pi_subset, pi_ii_subset, pi_jj_subset]\n",
    "pi_subset_full = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_subset_full =pi_subset_full.sort_values(['Country1', 'Country2', 'Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we actually just need pi_US_US (28 28) and pi_US_China (28 5)\n",
    "pi_for_calc12 = pi_subset_full[(pi_subset_full['Country1']==c1_ind) & (pi_subset_full['Country2']==c2_ind)]\n",
    "frames = [pi_ii_subset, pi_for_calc12]\n",
    "pi_for_calc = pd.concat(frames)\n",
    "pi_for_calc12 = pi_for_calc12.reset_index()\n",
    "pi_for_calc12.head()\n",
    "pi_ii_subset = pi_ii_subset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now subset d_hat in same way as pi\n",
    "d_data_subset = d_data_hat.drop(['d21', 'd22'],axis=1)\n",
    "d_data_subset = d_data_subset.drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now calculate p_hat_d\n",
    "def calc_p_hat(beta_L_for_calc, beta_K_for_calc):\n",
    "    theta_eknr = 2\n",
    "    p_D_hat = []\n",
    "    for i in range(0, 18):\n",
    "        pi_term_1 = pi_for_calc12.pi_value[i] #pi is 2000, 2001, etc.\n",
    "        pi_term_2 = pi_ii_subset.pi_value[i]\n",
    "        b_hat = calc_b_hat(beta_L_for_calc, beta_K_for_calc)\n",
    "        b_hat_term = b_hat[i+1] #bhat is 2000, 2001, etc.\n",
    "        d_hat_term_1 = d_data_subset.d12[i+1] #2000, 2001\n",
    "        d_hat_term_2 = d_data_subset.d11[i+1] #2000, 2001\n",
    "        A_hat_term = A_data_hat.country_1[i+1] #2000, 2001 \n",
    "\n",
    "        term_1 = pi_term_1 * ((b_hat_term*d_hat_term_1/A_hat_term))**(-1*theta_eknr)\n",
    "        term_2 = pi_term_2 * ((b_hat_term*d_hat_term_2/A_hat_term))**(-1*theta_eknr)\n",
    "        pd_term_temp = (term_1 + term_2)**(-1/theta_eknr)\n",
    "        p_D_hat.append(pd_term_temp)\n",
    "        \n",
    "    p_D_hat_df = pd.DataFrame(p_D_hat)\n",
    "    p_D_hat_df['Year'] = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "                '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "    p_D_hat_df.columns = ['p_D_hat', 'Year']\n",
    "    return(p_D_hat_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_X_pK_hat(beta_L_for_calc, beta_K_for_calc):\n",
    "    #Create the X/(pK) element, call it x_pK\n",
    "    X_pK_hat = []\n",
    "    for i in range(0, 18):\n",
    "        X_D_term_temp = inv_data_hat.Consumer_durables[i+1]#2000, 2001\n",
    "        p_D_hat_df = calc_p_hat(beta_L_for_calc, beta_K_for_calc)\n",
    "        p_D_term_temp = p_D_hat_df.p_D_hat[i]#2001\n",
    "        K_term_temp =k_data_hat.Consumer_durables[i+1]#2000\n",
    "        X_pK_temp = X_D_term_temp/(p_D_term_temp * K_term_temp)\n",
    "        X_pK_hat.append(X_pK_temp)\n",
    "\n",
    "    X_pK_hat_df = pd.DataFrame(X_pK_hat)\n",
    "    X_pK_hat_df['Year'] = ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "                '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "    X_pK_hat_df.columns = ['X_pK_hat', 'Year']\n",
    "    return(X_pK_hat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #merge everything i need into one dataframe\n",
    "#X_pK, chi, K\n",
    "# chi_data_hat = chi_data_hat.reset_index()\n",
    "# k_data_hat = k_data_hat.reset_index()\n",
    "# X_pK_hat_df = X_pK_hat_df.reset_index()\n",
    "\n",
    "def get_data(beta_L_for_calc, beta_K_for_calc, GDP_data):\n",
    "    #calculate Y_hat from GDP_data (only use durables)\n",
    "    GDP_raw.columns = ['GDP', 'Durable_goods', 'Services', 'G_private_dom_inv', 'Govt_cons_exp_and_gross_inv']\n",
    "    GDP_subset = GDP_raw['Durable_goods']\n",
    "\n",
    "    GDP_subset =pd.DataFrame(GDP_subset)\n",
    "    GDP_subset =GDP_subset.reset_index()\n",
    "    GDP_subset.columns = ['Year', 'Y']\n",
    "    GDP_subset['Y_hat'] = GDP_subset.Y.div(GDP_subset.Y.shift(1))\n",
    "\n",
    "    GDP_subset = GDP_subset.drop([0])\n",
    "    GDP_subset = GDP_subset.reset_index()\n",
    "\n",
    "    k_data_hat = k_data_hat_old.drop([0])\n",
    "    chi_data_hat = chi_data_hat_old.drop([0, 1])\n",
    "    k_data_hat = k_data_hat.reset_index()\n",
    "    chi_data_hat = chi_data_hat.reset_index()\n",
    "    X_pK_hat_df = calc_X_pK_hat(beta_L_for_calc, beta_K_for_calc)\n",
    "    X_pK_hat_df = X_pK_hat_df.reset_index()\n",
    "    lists_for_K_LOM = [pd.DataFrame(chi_data_hat['Country 1']),\n",
    "                      pd.DataFrame(k_data_hat['Consumer_durables']),\n",
    "                      pd.DataFrame(X_pK_hat_df['X_pK_hat']),\n",
    "                      pd.DataFrame(GDP_subset['Y_hat'])]\n",
    "    K_LOM_df = pd.concat(lists_for_K_LOM, axis=1)\n",
    "\n",
    "\n",
    "    K_LOM_df['Year']= ['2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "                '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "    K_LOM_df.columns = ['chi_hat', 'K_hat', 'X_pK_hat', 'Y_hat', 'Year']\n",
    "    return (K_LOM_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_data_mom= ['2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012',\n",
    "            '2013', '2014', '2015', '2016', '2017', '2018']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW MOMENTS\n",
    "# Define data moments - the capital in a given year\n",
    "from scipy import stats as sts\n",
    "def calc_data_moments(K_LOM_vals): #doesn't seem to need an argumentb but try for now\n",
    "#     data_moments = []\n",
    "    subset_no_recess = K_LOM_vals[(K_LOM_vals.Year.astype(int) <=2007)| (K_LOM_vals.Year.astype(int)>=2013)]\n",
    "    K_LOM_vals['K_hat_shift'] =K_LOM_vals.K_hat.shift(1) #for m3\n",
    "\n",
    "\n",
    "    #create a datamoment for each year, this is K_hat_t+2\n",
    "    data_m_1 = np.mean(subset_no_recess.K_hat) #avg K hat\n",
    "    data_m_2 = np.mean(subset_no_recess.K_hat/subset_no_recess.Y_hat)#Avg Khat/Y hat\n",
    "    data_m_3 = sts.pearsonr(K_LOM_vals.K_hat[1:len(K_LOM_vals)], K_LOM_vals.K_hat_shift.dropna())[0]#corr Kt+1, Kt\n",
    "#     data_m_4 = sum(subset_no_recess.K_hat >=1.03)/len(subset_no_recess.K_hat) #percentage above 1.03(around the mean)\n",
    "\n",
    "    data_moments = [data_m_1, data_m_2, data_m_3]#, data_m_4]\n",
    "    return data_moments\n",
    "\n",
    "\n",
    "def calc_model_moments(chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta):\n",
    "    \n",
    "    ##Compute K_hat_t+2\n",
    "    K_hat_path = []\n",
    "    #for very first year\n",
    "    K_hat_path.append(chi_hat[0] * (X_pK_hat[0]**alpha) * (K_hat[0] - (1-delta)) + (1-delta))\n",
    "    for i in range(1, 18):\n",
    "        K_t_2 = chi_hat[i] * (X_pK_hat[i]**alpha) * (K_hat_path[i-1] - (1-delta)) + (1-delta)\n",
    "        K_hat_path.append(K_t_2)\n",
    "    \n",
    "    subset_no_recess_K = K_hat_path[0:6] + K_hat_path[11:len(K_hat_path)] # a list\n",
    "#     print(\"percentile\",np.percentile(subset_no_recess_K, .5))\n",
    "    subset_no_recess_Y = np.concatenate((Y_hat[0:6],Y_hat[11:len(Y_hat)]))#these are data is that OK?\n",
    "#     print\n",
    "    # 0 = 2002\n",
    "    # 5 = 2007\n",
    "    # 11 = 2013\n",
    "    model_m_1 = np.mean(subset_no_recess_K)\n",
    "    model_m_2 = np.mean(subset_no_recess_K/subset_no_recess_Y)\n",
    "    model_m_3 = sts.pearsonr(K_hat_path[0:(len(K_hat_path)-1)], K_hat_path[1:len(K_hat_path)])[0] #is this just 1? print this\n",
    "#     model_m_4 = sum(pd.Series(subset_no_recess_K) >=1.03)/len(subset_no_recess_K)\n",
    "\n",
    "    model_moments = [model_m_1, model_m_2, model_m_3]#, model_m_4]\n",
    "    return model_moments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the error vector \n",
    "\n",
    "def err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta, simple = True):\n",
    "    data_moments = np.asarray(calc_data_moments(K_LOM_vals)) # 4 0's\n",
    "    model_moments = np.asarray(calc_model_moments(chi_hat, K_hat, X_pK_hat, Y_hat, alpha,delta))\n",
    "    \n",
    "    if simple:\n",
    "        err_vec = model_moments - data_moments\n",
    "    else:\n",
    "        err_vec = (model_moments - data_moments) / data_moments\n",
    "    return err_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define criterion function\n",
    "def criterion(params, *args):\n",
    "    alpha, delta = params\n",
    "    K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, W = args\n",
    "    err = err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta, simple=True)\n",
    "    crit_val = err.T @ W @ err\n",
    "    return crit_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options\n",
    "beta_L_EKNR = 2/3\n",
    "beta_K_EKNR = 1-beta_L_EKNR\n",
    "beta_L_gmm = .6720710743684\n",
    "beta_K_gmm = .3279289256316\n",
    "\n",
    "K_LOM_df_EKNR = get_data(beta_L_EKNR, beta_K_EKNR, GDP_raw)\n",
    "K_LOM_df_mine = get_data(beta_L_gmm, beta_K_gmm, GDP_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_hat= 0.47544915941430477 delta_hat= 0.1224845905934092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: 0.011678769286144681\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-5.6378513e-08,  0.0000000e+00])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 45\n",
       "      nit: 13\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.47544916, 0.12248459])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "##results when EKNR estimates of betas are used\n",
    "#Estimate through GMM\n",
    "# #initial guesses\n",
    "alpha_init = .5\n",
    "delta_init=.5\n",
    "\n",
    "params_init = np.array([alpha_init, delta_init])\n",
    "W_hat = np.eye(3) \n",
    "\n",
    "gmm_args = (K_LOM_df_EKNR, K_LOM_df_EKNR.chi_hat.values, K_LOM_df_EKNR.K_hat.values, K_LOM_df_EKNR.X_pK_hat.values, K_LOM_df_EKNR.Y_hat.values, W_hat)#1 is arbitrary, don't really need this arg\n",
    "results = opt.minimize(criterion, params_init, args=(gmm_args), tol=1e-14,\n",
    "                       method='L-BFGS-B',\n",
    "                       bounds=((1e-10, 1+(1e-10)), (1e-10, 1+(1e-10))))\n",
    "alpha_gmm, delta_gmm = results.x\n",
    "print('alpha_hat=', alpha_gmm, 'delta_hat=', delta_gmm)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_hat= 0.4746537617161641 delta_hat= 0.12207918631588566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: 0.011698356650983809\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-5.72458747e-08,  5.42968448e-08])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 96\n",
       "      nit: 14\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.47465376, 0.12207919])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##results when my estimates of betas are used\n",
    "#initial guesses\n",
    "# alpha_init=best[0]\n",
    "# delta_init=best[1] #given\n",
    "\n",
    "params_init = np.array([alpha_init, delta_init])\n",
    "W_hat = np.eye(3) \n",
    "\n",
    "gmm_args = (K_LOM_df_mine, K_LOM_df_mine.chi_hat.values, K_LOM_df_mine.K_hat.values, K_LOM_df_mine.X_pK_hat.values, K_LOM_df_mine.Y_hat.values, W_hat)#1 is arbitrary, don't really need this arg\n",
    "results = opt.minimize(criterion, params_init, args=(gmm_args), tol=1e-14,\n",
    "                       method='L-BFGS-B',\n",
    "                       bounds=((1e-10, 1+(1e-10)), (1e-10, 1+(1e-10))))\n",
    "alpha_gmm, delta_gmm = results.x\n",
    "print('alpha_hat=', alpha_gmm, 'delta_hat=', delta_gmm)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #nontarget moments\n",
    "# #Define data moments - the capital in a given year\n",
    "# def calc_data_moments_nontarget(K_LOM_vals): #doesn't seem to need an argumentb but try for now\n",
    "#     data_moments = []\n",
    "#     subset_no_recess = K_LOM_vals[(K_LOM_vals.Year.astype(int) <=2007)| (K_LOM_vals.Year.astype(int)>=2013)]\n",
    "\n",
    "\n",
    "#     data_m_4 = sum(subset_no_recess.K_hat >=1.03)/len(subset_no_recess.K_hat) #percentage above 1.03(around the mean)\n",
    "\n",
    "#     data_moments.append(data_m_4)\n",
    "#     #create a datamoment for each year, this is K_hat_t+2\n",
    "# #     for i in range(0,18):\n",
    "# #         data_moments.append(K_LOM_vals.K_hat[i])\n",
    "#     # create variance data moment\n",
    "#     data_moments.append(np.var(subset_no_recess.K_hat))\n",
    "#     return data_moments\n",
    "\n",
    "# def calc_model_moments_nontarget(chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta):\n",
    "#     model_moments2=[]\n",
    "#     ##Compute K_hat_t+2\n",
    "#     K_hat_path = []\n",
    "#     #for very first year\n",
    "#     K_hat_path.append(chi_hat[0] * (X_pK_hat[0]**alpha) * (K_hat[0] - (1-delta)) + (1-delta))\n",
    "#     for i in range(1, 18):\n",
    "#         K_t_2 = chi_hat[i] * (X_pK_hat[i]**alpha) * (K_hat_path[i-1] - (1-delta)) + (1-delta)\n",
    "#         K_hat_path.append(K_t_2)\n",
    "    \n",
    "#     subset_no_recess_K = K_hat_path[0:6] + K_hat_path[11:len(K_hat_path)] # a list\n",
    "#     model_m_4 = sum(pd.Series(subset_no_recess_K) >=1.03)/len(subset_no_recess_K)\n",
    "#     model_moments2.append(model_m_4)\n",
    "\n",
    "#     # create variance data moment\n",
    "#     model_moments2.append(np.var(subset_no_recess_K))\n",
    "    \n",
    "#     return model_moments2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_nontarget = calc_data_moments_nontarget(K_LOM_df_mine)\n",
    "# model_nontarget =  calc_model_moments_nontarget(K_LOM_df_mine.chi_hat.values, K_LOM_df_mine.K_hat.values, K_LOM_df_mine.X_pK_hat.values, K_LOM_df_mine.Y_hat.values, alpha_gmm, delta_gmm)\n",
    "# diff_m4 = data_nontarget[0] - model_nontarget[0]\n",
    "# diff_var = data_nontarget[1] - model_nontarget[1]\n",
    "# diff_m4\n",
    "# diff_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute SEs\n",
    "#he did a 2-sided numerical derivative, which is the expression above \"5. examples\"\n",
    "def Jac_err2(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta, simple = False):\n",
    "    '''\n",
    "    This function computes the Jacobian matrix of partial derivatives of the R x 1 moment\n",
    "    error vector e(x|theta) with respect to the K parameters theta_i in the K x 1 parameter vector\n",
    "    theta. The resulting matrix is R x K Jacobian.\n",
    "    '''\n",
    "    R = 3\n",
    "    K = 2\n",
    "    Jac_err = np.zeros((R, K))\n",
    "    h_alpha = 1e-8 * alpha\n",
    "    print(h_alpha)\n",
    "    h_delta = 1e-8 * delta\n",
    "    \n",
    "    # def err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta, simple = True):\n",
    "\n",
    "\n",
    "    \n",
    "    alpha_upper = alpha +h_alpha\n",
    "    alpha_lower = alpha -h_alpha\n",
    "    Jac_err[:, 0] = \\\n",
    "        ((err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha_upper,  delta, simple) -\n",
    "          err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha_lower, delta, simple)) / (2 * (1e-8)*(alpha))).flatten()\n",
    "    #.flatten: the jac-err is a 2x2, a slice that is a whole row turns it into a 1d object, so we need to\n",
    "    #fill it with a 1d object.\n",
    "    Jac_err[:, 1] = \\\n",
    "        ((err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta + h_delta, simple) -\n",
    "          err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat,  Y_hat, alpha, delta - h_delta, simple)) / (2 * h_delta)).flatten()\n",
    "    \n",
    "    return Jac_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as lin #good for inverse\n",
    "\n",
    "N = len(K_LOM_df_mine)\n",
    "d_err2 = Jac_err2(K_LOM_df_mine, K_LOM_df_mine.chi_hat.values, K_LOM_df_mine.K_hat.values, K_LOM_df_mine.X_pK_hat.values,K_LOM_df_mine.Y_hat.values, alpha_gmm, delta_gmm, True)\n",
    "# print(d_err2)\n",
    "SigHat2 = (1 / N) * lin.inv(d_err2.T @ W_hat @ d_err2)\n",
    "# print(SigHat2)\n",
    "print('Std. err. alpha_hat=', np.sqrt(SigHat2[0, 0]))\n",
    "print('Std. err. delta_hat=', np.sqrt(SigHat2[1, 1]))\n",
    "#this leadsto huge standard errors. When i do this with W_hat2, it is much lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the version using the second weighting matrix does not converge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_err2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define criterion function\n",
    "def criterion2(params, args):\n",
    "    alpha, delta = params\n",
    "    K_LOM_vals, chi_hat, K_hat, X_pK_hat, W = args\n",
    "    err = err_vec(K_LOM_vals, chi_hat, K_hat, X_pK_hat, alpha, delta, simple=True)\n",
    "    crit_val = err.T @ W @ err\n",
    "    return crit_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy.linalg as lin\n",
    "\n",
    "\n",
    "# #perform a grid search, as initial values not so good\n",
    "# ###Again, assume alpha and delta are parameters of interest (denoted a and d here). Similar intuition, except\n",
    "# ###uses ParameterGrid instead of nested loops over values of alpha and delta\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# def obj(params):\n",
    "#     a = params[0]\n",
    "#     d = params[1]\n",
    "#     params_0 = np.array([a, d])\n",
    "#     gmm_args_0 = (K_LOM_df_mine, K_LOM_df_mine.chi_hat.values, K_LOM_df_mine.K_hat.values, K_LOM_df_mine.X_pK_hat.values, W_hat2)#1 is arbitrary, don't really need this arg\n",
    "\n",
    "\n",
    "#     lik = criterion2(params_0, args=(gmm_args_0))\n",
    "#     return -lik\n",
    "# # Grid search to pick start location\n",
    "# param_grid = {'alpha': np.arange(0, 1, .01), 'delta': np.arange(0, 1, .01)}\n",
    "# grid = ParameterGrid(param_grid)\n",
    "# best = [0, 0]\n",
    "# best_obj = -1e10\n",
    "# for params in grid:\n",
    "#     a = params[\"alpha\"]\n",
    "#     d = params[\"delta\"]\n",
    "#     params_0 = np.array([a, d])\n",
    "#     gmm_args_0 = np.array([K_LOM_df, K_LOM_df.chi_hat.values, K_LOM_df.K_hat.values, K_LOM_df.X_pK_hat.values, W_hat2])#1 is arbitrary, don't really need this arg\n",
    "\n",
    "\n",
    "\n",
    "#     lik = criterion2(params_0, gmm_args_0)\n",
    "#     if lik > best_obj:\n",
    "#         best_obj = lik\n",
    "#         best = [a, d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat with different W\n",
    "import numpy.linalg as lin\n",
    "\n",
    "\n",
    "def get_err_mat(K_LOM_vals, chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta, simple = True):\n",
    "    R = 3 #number of moments\n",
    "    S = 18 # of obs\n",
    "    Err_mat = np.zeros((R, S))\n",
    "    model_points = calc_model_moments(chi_hat, K_hat, X_pK_hat, Y_hat, alpha, delta) #42 values\n",
    "    data_points = calc_data_moments(K_LOM_vals) #42 values\n",
    "    if simple:\n",
    "        for i in range(0, 3): #for i in range (0, 42) #number of moments\n",
    "            Err_mat[i, :] = data_points[i] - model_points[i]\n",
    "#             print(data_points[i])\n",
    "#             print(model_points[i])\n",
    "    else:\n",
    "        for i in range(0, 3): #for i in range (0, 42)\n",
    "            Err_mat[i, :] = (data_points[i] - model_points[i])/model_points[i]\n",
    "            \n",
    "    return Err_mat\n",
    "\n",
    "Err_mat = get_err_mat(K_LOM_df_mine, K_LOM_df_mine.chi_hat.values, K_LOM_df_mine.K_hat.values, K_LOM_df_mine.X_pK_hat.values, K_LOM_df_mine.Y_hat.values, alpha_gmm, delta_gmm,False)\n",
    "VCV2 = (1 / len(K_LOM_df_mine)) * (Err_mat @ Err_mat.T)\n",
    "W_hat2 = lin.inv(VCV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Err_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial guesses\n",
    "# alpha_init2 = best[0]\n",
    "# delta_init2 = best[1]\n",
    "alpha_init2=.4\n",
    "delta_init2=.09 #given\n",
    "\n",
    "params_init2 = np.array([alpha_init2, delta_init2])\n",
    "gmm_args2 = (K_LOM_df_mine, K_LOM_df_mine.chi_hat.values, K_LOM_df_mine.K_hat.values, K_LOM_df_mine.X_pK_hat.values, K_LOM_df_mine.Y_hat.values, W_hat2)#1 is arbitrary, don't really need this arg\n",
    "\n",
    "results = opt.minimize(criterion, params_init2, args=(gmm_args2), tol=1e-18,\n",
    "                       method='L-BFGS-B',\n",
    "                       bounds=((1e-10, 1-(1e-10)), (1e-10, 1-(1e-10))), options={'ftol': -1e-40, 'gtol': 1e-20})\n",
    "alpha_gmm2, delta_gmm2 = results.x\n",
    "print('alpha_hat2=', alpha_gmm2, 'delta_hat2=', delta_gmm2)\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
